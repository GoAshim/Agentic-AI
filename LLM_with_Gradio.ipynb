{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d22555",
   "metadata": {},
   "source": [
    "## LLM with Gradio\n",
    "\n",
    "Earlier we learned how to call different LLMs from python code. We also learned how to build UI application with very little coding using Gradio framework. Today we will combine those two together, by building simple Gen AI application with UI and perform various generative tasks by utilizing LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa12b3",
   "metadata": {},
   "source": [
    "### Plan\n",
    "In this exercise, we are going to build the following -\n",
    "\n",
    "* Connect UI with the LLM\n",
    "    * [Simple Q&A application](#Simple-q&a-application)\n",
    "    * [Q&A app with markdown response](#Q&A-application-with-markdown-response)\n",
    "    * [Q&A app with LLM of choice](#Q&A-application-with-llm-of-choice)\n",
    "    * [Simple chatbot application](#Simple-chatbot-application)\n",
    "    * [Simple chatbot with streamed output](#Simple-chatbot-with-streamed-output)\n",
    "    * [Chatbot with tool Calling](#Chatbot-with-tool-Calling)\n",
    "    * [Chatbot with tool calling (Enhanced Version)](#Chatbot-with-tool-calling-Enhanced-Version)\n",
    "    * Handle multiple tools call from AI assistant\n",
    "\n",
    "\n",
    "### Implementation - to be carried out through the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9547d5b",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77517de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# To call the OpenAI library which can call any commercial or open-source LLMs\n",
    "from openai import OpenAI\n",
    "\n",
    "# To display the output in nice format instead of plain text\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# To scrape the content from a website\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# To build the UI\n",
    "import gradio as gr\n",
    "\n",
    "# To parse the JSON response from LLM (which we need for tool calling in chatbot)\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be30662",
   "metadata": {},
   "source": [
    "#### Get the API Key from the environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b969ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key found and looks good so far!\n",
      "Google API key found and looks good so far!\n",
      "OpenRouter API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1\" # This is the default base url for OpenAI library\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not OPENAI_API_KEY.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif OPENAI_API_KEY.strip() != OPENAI_API_KEY:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"OpenAI API key found and looks good so far!\")\n",
    "\n",
    "# Google\n",
    "GOOGLE_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Validate the Google API key\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not GOOGLE_API_KEY.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"Google API key found and looks good so far!\")\n",
    "\n",
    "# OpenRouter\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Validate the OpenRouter API key\n",
    "if not OPENROUTER_API_KEY:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "else:\n",
    "    print(\"OpenRouter API key found and looks good so far!\")\n",
    "\n",
    "\n",
    "# Ollama running locally\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\" \n",
    "OLLAMA_API_KEY = \"NA\" # the api_key is not relevant as the model is running locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017258c",
   "metadata": {},
   "source": [
    "#### Simple Q&A application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2216546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"You are a helpful assistant, always answer in a courteous manner.\"\n",
    "\n",
    "# Define the custom function\n",
    "def first_chatbot(user_prompt):\n",
    "    # Instantiate OpenAI with proper API key\n",
    "    openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Call the LLM with the system and user prompt\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=first_chatbot,\n",
    "    title=\"First Chatbot\",\n",
    "    inputs=gr.Textbox(label=\"User prompt\"),\n",
    "    outputs=gr.Textbox(label=\"Response from LLM\", lines=10)\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616f6b6",
   "metadata": {},
   "source": [
    "#### Q&A application with markdown response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c70fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"You are a helpful assistant, always answer in a courteous manner.\"\n",
    "\n",
    "# Define the custom function\n",
    "def first_chatbot(user_prompt):\n",
    "    # Instantiate OpenAI with proper API key\n",
    "    openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Call the LLM with the system and user prompt\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=first_chatbot,\n",
    "    title=\"First Chatbot\",\n",
    "    inputs=gr.Textbox(label=\"User prompt\", lines=5),\n",
    "    outputs=gr.Markdown(label=\"Response from LLM\") # This is the only change from the previous example\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9dd223",
   "metadata": {},
   "source": [
    "#### Q&A application with LLM of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an expert in latest technologies. \n",
    "Provide detailed explanation. \n",
    "Do not assume user has any prior knowledge.\n",
    "\"\"\"\n",
    "\n",
    "# Define the custom function\n",
    "def chatbot_with_llm_of_choice(user_prompt, user_model):\n",
    "    # Instantiate OpenAI with proper API key\n",
    "    if user_model == \"GPT from OpenAI\":\n",
    "        model_api = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "        model=\"gpt-4.1-nano\"\n",
    "    elif user_model == \"Gemini from Google\":\n",
    "        model_api = OpenAI(api_key=GOOGLE_API_KEY, base_url=GOOGLE_BASE_URL)\n",
    "        model=\"gemini-2.5-flash-lite\"\n",
    "    elif user_model == \"Mistral Devstral via OpenRouter\":\n",
    "        model_api = OpenAI(api_key=OPENROUTER_API_KEY, base_url=OPENROUTER_BASE_URL)\n",
    "        model=\"mistralai/devstral-2512:free\"\n",
    "    elif user_model == \"Deepseek from Ollama at Local\":\n",
    "        model_api = OpenAI(api_key=OLLAMA_API_KEY, base_url=OLLAMA_BASE_URL)\n",
    "        model=\"deepseek-r1:1.5b\"\n",
    "    else:\n",
    "        return \"Invalid model\"\n",
    "\n",
    "    # Call the LLM with the system and user prompt\n",
    "    response = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=chatbot_with_llm_of_choice,\n",
    "    title=\"Chatbot with LLM of Choice\",\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"User prompt\", lines=5),\n",
    "        gr.Dropdown(\n",
    "            label=\"Select LLM\", \n",
    "            value=\"GPT from OpenAI\",\n",
    "            choices=[\n",
    "                \"GPT from OpenAI\", \n",
    "                \"Gemini from Google\",\n",
    "                \"Mistral Devstral via OpenRouter\",\n",
    "                \"Deepseek from Ollama at Local\"\n",
    "            ] \n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Markdown(label=\"Response from LLM\")\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef32647",
   "metadata": {},
   "source": [
    "#### Simple chatbot application\n",
    "\n",
    "So far we have built Q&A application and ran few use cases, where user can enter their question on the left, and the LLM generates answer on the right. Now, we are going to build a chatbot, which will be like a flow of conversation between question from user and response from LLM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c994a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"You are a helpful assistant, always answer in a courteous manner.\"\n",
    "\n",
    "# Define the custom function\n",
    "def first_chatbot(user_prompt, context_history):\n",
    "    \n",
    "    # Instantiate OpenAI with proper API key\n",
    "    model_api = OpenAI(api_key=OPENROUTER_API_KEY, base_url=OPENROUTER_BASE_URL)\n",
    "    model=\"mistralai/devstral-2512:free\"\n",
    "\n",
    "    # Extract 'role' and 'context' values from the conversation history and build a dictionary\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in context_history]\n",
    "    # Create the new message list with system prompt, conversation history and the new user prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Call the LLM with the system and user prompt\n",
    "    response = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.ChatInterface(fn=first_chatbot, type=\"messages\", title=\"First ChatBot\")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125af022",
   "metadata": {},
   "source": [
    "The Chatbot above with the free mistralai model via OpenRouter did great. Where I started with my first question *\"What are the top 10 companies in the world in terms of revenue?\"* and it gave the correct answer. Then I asked *\"How do these companies rank in terms of profit?\"* and the LLM responded with the profit data and comparison between the same 10 companies from the answer of the first question. If we didn't feed the context history then the model would have produced top companies in terms of profit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a0557",
   "metadata": {},
   "source": [
    "#### Simple chatbot with streamed output\n",
    "\n",
    "We will build very similar chatbot as above but add the streaming effect, so the user will see a stream of text as the LLM is generating output, instead of waiting with a blank screen till the model finished generating the entire content of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5484813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "from email import message\n",
    "\n",
    "\n",
    "system_prompt = \"You are a helpful assistant, always answer in courteous manner and in simple language\"\n",
    "\n",
    "# Define the custom function\n",
    "def first_chatbot(user_prompt, context_history):\n",
    "    \n",
    "    # Instantiate OpenAI with proper API key\n",
    "    model_api = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "    model=\"gpt-4.1-mini\"\n",
    "\n",
    "    # Extract 'role' and 'context' values from the conversation history and build a dictionary\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in context_history]\n",
    "    # Create the new message list with system prompt, conversation history and the new user prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Call the LLM with the combined prompt\n",
    "    stream = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True # Change from previous example, this makes the API to return token incrementally\n",
    "    )\n",
    "\n",
    "    # Capture the incremental response and send to the UI\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or '' # instead of message.content we use delta.content to get the change\n",
    "        yield response # the 'yield' sends the incremental response to Gradio, unlike 'return' which sends the final response at the end\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.ChatInterface(fn=first_chatbot, type=\"messages\", title=\"First ChatBot\")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72c362",
   "metadata": {},
   "source": [
    "#### Interesting Observation\n",
    "\n",
    "I first tried the chatbot above with streamed output by using the *mistralai/devstral-2512:free* model from *OpenRouter*. However that ran into error while generating content. Then I switched to *gpt-4.1-mini* model and the chatbot worked perfectly fine with multiple prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e4efc",
   "metadata": {},
   "source": [
    "#### Chatbot with tool calling\n",
    "\n",
    "We have built the regular chatbot functionality where LLM answers to user prompt. Now, along with that, we will let LLM to decide whether there is a need to call a tool (which will be a simple function to begin with). Then the UI (Gradio) will send the response from the LLM indicating the need for a tool (the custom function) call to the backend code. The backend code will parse the response, call the tool (the custom function), and send the response along with the output generated by the custom function (tool). Then the LLM will parse that message and respond to the user.\n",
    "\n",
    "So, there will be some additional steps when we need to code a chatbot with tool calling capability, over simple chatbots which we have built earlier. Those steps are as follows -\n",
    "1. Define the tool, which is a custom function to perform any task we would like.\n",
    "2. Create a JSON in a certain way, which will tell the LLM about what tool needs to be called in which condition.\n",
    "3. Pass that JSON as a new parameter called *tool* while calling the LLMs *chat.completions.create* function\n",
    "4. Create a new function, which will \n",
    "    * Parse the response from LLM, \n",
    "    * Evaluate if LLM indicated any tool call\n",
    "    * If so then will call the custom function (tool)\n",
    "    * Create the output in specific format\n",
    "    * We can customize this function if we may need to call different custom functions (tools) in future.\n",
    "5. Combine the response from the new function and the original LLM response and send that to the LLM for second time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly travel booking assistant. Please answer in one sentence. \n",
    "For any non travel related question please respond saying that\n",
    "Sorry I do not have the information, However I can help you related to travel booking\n",
    "\"\"\"\n",
    "\n",
    "# *** Begin - Custom Funtion (Tool) ***\n",
    "\n",
    "# Dictionary with ticket price to different cities - we can modify this to find from a DB table\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "# Function to get the ticket price of a given city\n",
    "def get_ticket_price(destination_city):\n",
    "    # The following print statement is optional to show when tool calling is happening in code output\n",
    "    print(f\"Tool called for city {destination_city}\")\n",
    "    price = ticket_prices.get(destination_city.lower(), \"Unknown ticket price\")\n",
    "    return f\"The price of a ticket to {destination_city} is {price}\"\n",
    "\n",
    "# *** End - Custom Funtion (Tool) ***\n",
    "\n",
    "\n",
    "# *** Begin - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\", # The name of the custom function (tool)\n",
    "    \"description\": \"Get the price of a return ticket to the destination city.\",\n",
    "    \"parameters\": { # Define each parameters required by the tool function, we have only 1 called destination_city\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\", # Data type of the input parameter\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]\n",
    "# *** End - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "\n",
    "# *** Begin - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    if tool_call.function.name == \"get_ticket_price\":\n",
    "        # Parse the arguments for the tool function from the message\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        city = arguments.get('destination_city')\n",
    "        # Call the tool function\n",
    "        price_details = get_ticket_price(city)\n",
    "        # Generate the response in following JSON format\n",
    "        response = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": price_details,\n",
    "            \"tool_call_id\": tool_call.id\n",
    "        }\n",
    "    return response\n",
    "\n",
    "# *** End - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "# Define the chat function\n",
    "def chatbot_with_tool(user_prompt, context_history):\n",
    "    \n",
    "    # Instantiate OpenAI with proper API key\n",
    "    model_api = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "    model=\"gpt-4.1-mini\"\n",
    "\n",
    "    # Extract 'role' and 'context' values from the conversation history and build a dictionary\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in context_history]\n",
    "    # Create the new message list with system prompt, conversation history and the new user prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Make the FIRST call to the LLM with the system prompt, user prompt, and the chat history\n",
    "    response = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools # This is the new parameter with all the info of custom function (tool) to the LLM\n",
    "    )\n",
    "\n",
    "    # Evaluate if the LLM response indicated any need of tool call\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_response = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(tool_response)\n",
    "\n",
    "        # Make the SECOND call to the LLM with the entire message thread combined with the response from the tool\n",
    "        # Note that this time we are not passing the third parameter 'tools=tools', because we just want the LLM\n",
    "        # to summarise the entire message and the response from tool and generate a nice output to the user.\n",
    "        response = model_api.chat.completions.create(model=model, messages=messages)\n",
    "    else:\n",
    "        # The following print statement is optional to show when tool calling is happening in code output\n",
    "        print(\"No tool called yet\")\n",
    "\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.ChatInterface(fn=chatbot_with_tool, type=\"messages\", title=\"ChatBot with Tool Calling\")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb03237",
   "metadata": {},
   "source": [
    "#### Chatbot with tool calling (Enhanced Version)\n",
    "\n",
    "We have built the chatbot functionality with ability to call custom function or tool. However, when we try to call the tool more than once, such as for prompt like 'I am thinking to go to either London or Paris, which option will be cheaper'. Here the tool needs to be called twice, once to get the ticket price for London and then again for Paris. In such case the above code will run into error, as it's created in a way to call the tool only once. \n",
    "\n",
    "We are going to fix the issue by making a simple change in the custom function - *handle_tool_call*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ae524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly travel booking assistant. Please answer in one sentence. \n",
    "For any non travel related question please respond saying that\n",
    "Sorry I do not have the information, However I can help you related to travel booking\n",
    "\"\"\"\n",
    "\n",
    "# *** Begin - Custom Funtion (Tool) ***\n",
    "\n",
    "# Dictionary with ticket price to different cities - we can modify this to find from a DB table\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "# Function to get the ticket price of a given city\n",
    "def get_ticket_price(destination_city):\n",
    "    # The following print statement is optional to show when tool calling is happening in code output\n",
    "    print(f\"Tool called for city {destination_city}\")\n",
    "    price = ticket_prices.get(destination_city.lower(), \"Unknown ticket price\")\n",
    "    return f\"The price of a ticket to {destination_city} is {price}\"\n",
    "\n",
    "# *** End - Custom Funtion (Tool) ***\n",
    "\n",
    "\n",
    "# *** Begin - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\", # The name of the custom function (tool)\n",
    "    \"description\": \"Get the price of a return ticket to the destination city.\",\n",
    "    \"parameters\": { # Define each parameters required by the tool function, we have only 1 called destination_city\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\", # Data type of the input parameter\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": price_function}]\n",
    "# *** End - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "\n",
    "# *** Begin - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "def handle_tool_call_adv(message):\n",
    "    response = [] # Define an empty list\n",
    "\n",
    "    for tool_call in message.tool_calls: # This is the change from above to loop through to handle multiple tool calls\n",
    "        if tool_call.function.name == \"get_ticket_price\":\n",
    "            # Parse the arguments for the tool function from the message\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            city = arguments.get('destination_city')\n",
    "            # Call the tool function\n",
    "            price_details = get_ticket_price(city)\n",
    "            # Generate the response in the following JSON format and append to the list\n",
    "            response.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": price_details,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    return response\n",
    "\n",
    "# *** End - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "# Define the chat function\n",
    "def chatbot_with_tool_adv(user_prompt, context_history):\n",
    "    \n",
    "    # Instantiate OpenAI with proper API key\n",
    "    model_api = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "    model=\"gpt-4.1-mini\"\n",
    "\n",
    "    # Extract 'role' and 'context' values from the conversation history and build a dictionary\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in context_history]\n",
    "    # Create the new message list with system prompt, conversation history and the new user prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Make the FIRST call to the LLM with the system prompt, user prompt, and the chat history\n",
    "    response = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools # This is the new parameter with all the info of custom function (tool) to the LLM\n",
    "    )\n",
    "\n",
    "    # Evaluate if the LLM response indicated any need of tool call\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_responses = handle_tool_call_adv(message)\n",
    "        messages.append(message)\n",
    "        # Instead of 'append', we use 'extend' to add each element from tool response separately\n",
    "        # This is essential to allow tool calls more than once in a single prompt. \n",
    "        # messages.append(tool_response)\n",
    "        messages.extend(tool_responses)\n",
    "\n",
    "        # Make the SECOND call to the LLM with the entire message thread combined with the response from the tool\n",
    "        # Note that this time we are not passing the third parameter 'tools=tools', because we just want the LLM\n",
    "        # to summarise the entire message and the response from tool and generate a nice output to the user.\n",
    "        response = model_api.chat.completions.create(model=model, messages=messages)\n",
    "    else:\n",
    "        # The following print statement is optional to show when tool calling is happening in code output\n",
    "        print(\"No tool called yet\")\n",
    "\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.ChatInterface(fn=chatbot_with_tool_adv, type=\"messages\", title=\"ChatBot with Tool Calling\")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db32b2fe",
   "metadata": {},
   "source": [
    "#### Chatbot with multiple tool calling\n",
    "\n",
    "We have built the chatbot functionality with ability to call custom function or tool. Now we will make couple of modifications, such as -\n",
    "* We are going to add multiple custom functions or tools for LLM to call as needed based on user prompt.\n",
    "* We will do some basic database operations in the tools functions.\n",
    "\n",
    "So, we are going to make some modifications in the code we wrote above, such as -\n",
    "1. Define the custom function for each tool with necessary database operation.\n",
    "2. Create JSON dictionary for each tool for LLM to understand.\n",
    "3. Modify the custom function, which will \n",
    "    * Parse the response from LLM.\n",
    "    * Evaluate if LLM indicated any tool call.\n",
    "    * Decide which tool to call based on the response.\n",
    "    * If so then will call the custom function (tool).\n",
    "    * Create the output in specific format.\n",
    "4. Combine the response from the new function and the original LLM response and send that to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are a friendly travel booking assistant. Please answer in one sentence. \n",
    "For any non travel related question please respond saying that\n",
    "Sorry I do not have the information, However I can help you related to travel booking.\n",
    "\"\"\"\n",
    "\n",
    "# *** Begin - First Custom Funtion (Tool) ***\n",
    "\n",
    "# Dictionary with ticket price to different cities - we can modify this to find from a DB table\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "# Function to get the ticket price of a given city\n",
    "def get_ticket_price(destination_city):\n",
    "    # The following print statement is optional to show when tool calling is happening in code output\n",
    "    print(f\"Price check tool called for city {destination_city}\")\n",
    "    price = ticket_prices.get(destination_city.lower(), \"Unknown ticket price\")\n",
    "    return f\"The price of a ticket to {destination_city} is {price}\"\n",
    "\n",
    "# *** End - First Custom Funtion (Tool) ***\n",
    "\n",
    "\n",
    "# *** Begin - Second Custom Funtion (Tool) ***\n",
    "\n",
    "# Dictionary with ticket price to different cities - we can modify this to find from a DB table\n",
    "ticket_info = {\"london\": \"L-0158\", \"paris\": \"P-9419\", \"tokyo\": \"T-8821\", \"berlin\": \"B-5219\"}\n",
    "\n",
    "# Function to get the ticket price of a given city\n",
    "def get_ticket_info(destination_city):\n",
    "    # The following print statement is optional to show when tool calling is happening in code output\n",
    "    print(f\"Booking information tool called for city {destination_city}\")\n",
    "    booking_number = ticket_info.get(destination_city.lower(), \"No booking info available\")\n",
    "    return f\"The price of a ticket to {destination_city} is {booking_number}\"\n",
    "\n",
    "# *** End - Second Custom Funtion (Tool) ***\n",
    "\n",
    "\n",
    "# *** Begin - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "# Dictionary for the first tool function\n",
    "price_function = {\n",
    "    \"name\": \"get_ticket_price\", # The name of the custom function (tool)\n",
    "    \"description\": \"Get the price of a return ticket to the destination city.\",\n",
    "    \"parameters\": { # Define each parameters required by the tool function, we have only 1 called destination_city\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\", # Data type of the input parameter\n",
    "                \"description\": \"The city that the customer wants to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Dictionary for the second tool function\n",
    "booking_function = {\n",
    "    \"name\": \"get_ticket_info\", # The name of the custom function (tool)\n",
    "    \"description\": \"Get the information of ticket booked to the destination city.\",\n",
    "    \"parameters\": { # Define each parameters required by the tool function, we have only 1 called destination_city\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"destination_city\": {\n",
    "                \"type\": \"string\", # Data type of the input parameter\n",
    "                \"description\": \"The city that the customer has booked ticket to travel to\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"destination_city\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create JSON with details of both the tools\n",
    "tools = [\n",
    "    {\"type\": \"function\", \"function\": price_function},\n",
    "    {\"type\": \"function\", \"function\": booking_function}\n",
    "]\n",
    "\n",
    "# *** End - Create a JSON, which will tell the LLM about what tool needs to be called in which condition.***\n",
    "\n",
    "\n",
    "# *** Begin - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    response = []\n",
    "\n",
    "    for tool_call in message.tool_calls:\n",
    "\n",
    "        # Check for the first tool call\n",
    "        if tool_call.function.name == \"get_ticket_price\":\n",
    "            # Parse the arguments for the tool function from the message\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            city = arguments.get('destination_city')\n",
    "            # Call the tool function\n",
    "            price_details = get_ticket_price(city)\n",
    "            # Generate the response in following JSON format\n",
    "            response.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": price_details,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "\n",
    "        # Check for the second tool call\n",
    "        if tool_call.function.name == \"get_ticket_info\":\n",
    "            # Parse the arguments for the tool function from the message\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            city = arguments.get('destination_city')\n",
    "            # Call the tool function\n",
    "            booking_details = get_ticket_info(city)\n",
    "            # Generate the response in following JSON format\n",
    "            response.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": booking_details,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "\n",
    "    return response\n",
    "\n",
    "# *** End - The function that parses the response fro first LLM call, call the tool function and get response from it\n",
    "\n",
    "# Define the chat function\n",
    "def chatbot_with_tool(user_prompt, context_history):\n",
    "    \n",
    "    # Instantiate OpenAI with proper API key\n",
    "    model_api = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "    model=\"gpt-4.1-mini\"\n",
    "\n",
    "    # Extract 'role' and 'context' values from the conversation history and build a dictionary\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in context_history]\n",
    "    # Create the new message list with system prompt, conversation history and the new user prompt\n",
    "    messages=[{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Make the FIRST call to the LLM with the system prompt, user prompt, and the chat history\n",
    "    response = model_api.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools # This is the new parameter with all the info of custom function (tool) to the LLM\n",
    "    )\n",
    "\n",
    "    # Evaluate if the LLM response indicated any need of tool call\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        tool_responses = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.extend(tool_responses)\n",
    "\n",
    "        # Make the SECOND call to the LLM with the entire message thread combined with the response from the tool\n",
    "        # Note that this time we are not passing the third parameter 'tools=tools', because we just want the LLM\n",
    "        # to summarise the entire message and the response from tool and generate a nice output to the user.\n",
    "        response = model_api.chat.completions.create(model=model, messages=messages)\n",
    "    else:\n",
    "        # The following print statement is optional to show when tool calling is happening in code output\n",
    "        print(\"No tool called yet\")\n",
    "\n",
    "\n",
    "\n",
    "    # Return the output generated by the LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Instantiate gradio interface\n",
    "interface = gr.ChatInterface(fn=chatbot_with_tool, type=\"messages\", title=\"ChatBot with Multiple Tools\")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
