{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Transcription Summarization and Translation Agent using Open Source LLM from HuggingFace\n",
        "\n",
        "This is the fourth notebook in the series of experiments where I will build different AI agents using open-source LLMs from HuggingFace. In this notebook, I will use multiple functionalities using LLMs and call them from Gradio interface -\n",
        "* Generate transcript of an audio file\n",
        "* Create a summarization of that transcript\n",
        "* Translate that summary to different language\n",
        "\n",
        "### Google Colab\n",
        "I will use Google Colab for creating and running the python code to build the AI agents using open-source LLMs from HuggingFace. Why did I choose Google Colab instead of my local computer?\n",
        "* Free access to powerful T4 GPUs needed to run most of the LLMs efficiently.\n",
        "* Easy ability to share code and collaborate.\n",
        "\n",
        "### Hugging Face\n",
        "I will need to connect to HuggingFace to use the appropriate open-source LLM for the AI application and connect that from my notebook in Colab. Here are the steps -\n",
        "* Create a free HuggingFace account at https://huggingface.co\n",
        "* Navigate to Settings from the user menu on the top right.\n",
        "* Create a new API token with **write** permissions.\n",
        "* Back to this colab notebook\n",
        "  * Press the \"key\" icon on the side panel to the left\n",
        "  * Click on add a new secret\n",
        "  * In the name field put HF_TOKEN\n",
        "  * In the value field put your actual token: hf_...\n",
        "  * Ensure the notebook access switch is turned ON.\n",
        "\n",
        "This way I can use my confidential API Keys for HuggingFace or others without needing to type them into my colab notebook, I will be sharing with others."
      ],
      "metadata": {
        "id": "kOIBb82S27cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and specifications, such as its memory usage, temperature, and clock speed.\n",
        "# We can also see that in details by clicking on Runtime (top menu) > View Resources\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "q84x9WTsADQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will need to connect from my notebook in Colab to HuggingFace by validating the token, in order to use open-source models.\n",
        "# The huggingface_hub library allows to interact with the HuggingFace Hub, a platform democratizing open-source LLMs and Datasets\n",
        "\n",
        "import os\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "378coGwPFpmD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive in Google Colab\n",
        "\n",
        "Google Colab allows us to access files stored in our Google Drive, making it easy to work with datasets and other resources. Here are the steps to mount Google Drive in Google Colab"
      ],
      "metadata": {
        "id": "g9tGFmm5ou3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2 - Authorize access through the prompts in the browser"
      ],
      "metadata": {
        "id": "uJ-QEF94olXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4fe89b-77ed-4da0-de71-83e87b3664f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection\n",
        "\n",
        "I will select a model from the HuggingFace model library based on the specific  application. Here are the steps -\n",
        "\n",
        "* Go to https://huggingface.co/models.\n",
        "* For the Speech Recognition / Transcription model -\n",
        "  * Click on **Automatic Speech Recognition** under Audio.\n",
        "  * Choose any model and review it's specification.\n",
        "  * I am choosing the **whisper-large** model from **OpenAI**\n",
        "* For the Text Summarization model -\n",
        "  * Click on **Summarization** under NLP.\n",
        "  * Choose any model and review it's specification.\n",
        "  * I am choosing the **bart-large-cnn** model from **Facebook**\n",
        "* For the Translation model -\n",
        "  * Click on **Translation** under NLP.\n",
        "  * Choose any model and review it's specification.\n",
        "  * I am choosing the **nllb-200-distilled-600M** model from **Facebook**\n",
        "\n",
        "Note: We should select a model based on various criteria, such as the specific use-casr, available infrastructure, latency, performance. I will cover those in details later."
      ],
      "metadata": {
        "id": "Mg2jfrVvH5db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Pipeline Library\n",
        "\n",
        "This is a much simpler approach with the Hugging Face pipeline API, which  provides a high-level, task-specific interface for running inference with pretrained models without manually handling tokenization, preprocessing, or postprocessing.\n",
        "\n",
        "This approach is ideal, when we need to run quick experimentation or prototyping and don't need to gain more granular control on the model behavior."
      ],
      "metadata": {
        "id": "U1CpegdxhTXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline for audio transcription\n",
        "transcriber = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-medium.en\",\n",
        "    dtype=torch.bfloat16,\n",
        "    device='cuda',\n",
        "    return_timestamps=True\n",
        ")"
      ],
      "metadata": {
        "id": "c-P2FTteKs6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline for text summarization\n",
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")"
      ],
      "metadata": {
        "id": "BSCNReTeld0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline for text translation\n",
        "translator = pipeline(\n",
        "    task=\"translation\",\n",
        "    model=\"facebook/nllb-200-distilled-600M\",\n",
        "    dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "nqR0tvk0laG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Custom Function to Call LLM\n",
        "\n",
        "We will build custom function to transcribe audio file, create a summary of that transcription, and translate that summary to a different language. We will use the respective pipelines we built before from the functions."
      ],
      "metadata": {
        "id": "e47LlcqGUJMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to generate the transcription\n",
        "def transcribe(audio_filename):\n",
        "  if audio_filename is None:\n",
        "    return \"No audio file provided\"\n",
        "  # Run inference - Generate the transcript text by calling the model through the pipeline API\n",
        "  transcript = transcriber(audio_filename)[\"text\"]\n",
        "  # Return the transcript text\n",
        "  return transcript"
      ],
      "metadata": {
        "id": "TgrRbQmjji0j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to summarize text\n",
        "def summarize(text_to_summarize):\n",
        "  if text_to_summarize is None:\n",
        "    return \"No text provided\"\n",
        "  # Run inference - Generate the text summarization by calling the model through the pipeline API\n",
        "  summary = summarizer(text_to_summarize)\n",
        "  # Return the summary text\n",
        "  return summary"
      ],
      "metadata": {
        "id": "krOTZXs0TvW7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to translate text\n",
        "def translate(text_to_translate, source_language, target_language):\n",
        "  if text_to_translate is None:\n",
        "    return \"No text provided\"\n",
        "  # Run inference - Generate the text translation by calling the model through the pipeline API\n",
        "  translation = translator(text_to_translate, src_lang=source_language, tgt_lang=target_language)\n",
        "  # Return the translated text\n",
        "  return translation"
      ],
      "metadata": {
        "id": "YLOGNuT-T55t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We stored the audio file under the root of 'My Drive' folder.\n",
        "audio_filename_1 = \"/content/drive/MyDrive/denver_extract.mp3\" # Audio with 15 minutes of conversation\n",
        "audio_filename_2 = \"/content/drive/MyDrive/little-girl.mp3\" # Small 15 seconds audio clip\n",
        "\n",
        "result = transcribe(audio_filename_2)\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "id": "t5cIdAZNesrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = summarize(result)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "Kp-CxvxnfcwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation = translate(summary, \"eng_Latn\", \"ben_Beng\")\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "VkrGFWyQfq4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application 2 - Display the audio transcript in Gradio UI"
      ],
      "metadata": {
        "id": "B-6pSNDmqZzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the UI with Gradio\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(\n",
        "        label=\"Upload audio file\",\n",
        "        sources=['upload'] # Indicate file upload, which is default, or it can be microphone\n",
        "        type=['filepath'] # Dedault is 'numpy' which converts the audio to a tuple consisting of the sample rate and the data\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Transcript\"),\n",
        "    title=\"Audio Transcription using Whisper from HuggingFace\",\n",
        "    description=\"Upload an audio file and get the transcript.\"\n",
        ")\n",
        "\n",
        "# Launch the UI\n",
        "ui.launch()"
      ],
      "metadata": {
        "id": "X1J3wz5d6Ovg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}