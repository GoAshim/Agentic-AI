{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Speech Recognition Agent using Open Source LLM from HuggingFace\n",
        "\n",
        "This is the third notebook in the series of experiments where I will build different AI agents using open-source LLMs from HuggingFace. In this notebook I will build AI agent to recognize speech from an audio file and create the transcript using open-source LLM from HuggingFace. I will also build a user interface with Gradio for user to be able to upload the audio file and see the transcript.\n",
        "\n",
        "### Google Colab\n",
        "I will use Google Colab for creating and running the python code to build the AI agents using open-source LLMs from HuggingFace. Why did I choose Google Colab instead of my local computer?\n",
        "1. Free access to powerful T4 GPUs needed to run most of the LLMs efficiently.\n",
        "2. Easy ability to share code and collaborate.\n",
        "\n",
        "### Hugging Face\n",
        "I will need to connect to HuggingFace to use the appropriate open-source LLM for the AI application and connect that from my notebook in Colab. Here are the steps -\n",
        "1. Create a free HuggingFace account at https://huggingface.co\n",
        "2. Navigate to Settings from the user menu on the top right.\n",
        "3. Create a new API token with **write** permissions.\n",
        "4. Back to this colab notebook\n",
        "  * Press the \"key\" icon on the side panel to the left\n",
        "  * Click on add a new secret\n",
        "  * In the name field put HF_TOKEN\n",
        "  * In the value field put your actual token: hf_...\n",
        "  * Ensure the notebook access switch is turned ON.\n",
        "\n",
        "This way I can use my confidential API Keys for HuggingFace or others without needing to type them into my colab notebook, I will be sharing with others."
      ],
      "metadata": {
        "id": "kOIBb82S27cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and specifications, such as its memory usage, temperature, and clock speed.\n",
        "# We can also see that in details by clicking on Runtime (top menu) > View Resources\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "q84x9WTsADQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will need to connect from my notebook in Colab to HuggingFace by validating the token, in order to use open-source models.\n",
        "# The huggingface_hub library allows to interact with the HuggingFace Hub, a platform democratizing open-source LLMs and Datasets\n",
        "\n",
        "import os\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import gradio as gr\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "378coGwPFpmD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive in Google Colab\n",
        "\n",
        "Google Colab allows us to access files stored in our Google Drive, making it easy to work with datasets and other resources. Here are the steps to mount Google Drive in Google Colab"
      ],
      "metadata": {
        "id": "g9tGFmm5ou3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 - Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2 - Authorize access through the prompts in the browser"
      ],
      "metadata": {
        "id": "uJ-QEF94olXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We stored the audio file under the root of 'My Drive' folder.\n",
        "audio_filename_1 = \"/content/drive/MyDrive/denver_extract.mp3\" # Audio with 15 minutes of conversation\n",
        "audio_filename_2 = \"/content/drive/MyDrive/little-girl.mp3\" # Small 15 seconds audio clip"
      ],
      "metadata": {
        "id": "ZMOf5ZP5qXmj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection\n",
        "\n",
        "I will select a model from the HuggingFace model library based on the specific  application. Here are the steps -\n",
        "\n",
        "* Go to https://huggingface.co/models.\n",
        "* Click on **Automatic Speech Recognition** under NLP.\n",
        "* Choose any model and review it's specification.\n",
        "* I am choosing the **whisper-large** model from OpenAI\n",
        "\n",
        "Note: We should select a model based on various criteria, such as the specific use-casr, available infrastructure, latency, performance. I will cover those in details later."
      ],
      "metadata": {
        "id": "Mg2jfrVvH5db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Pipeline Library\n",
        "\n",
        "This is a much simpler approach with the Hugging Face pipeline API, which  provides a high-level, task-specific interface for running inference with pretrained models without manually handling tokenization, preprocessing, or postprocessing.\n",
        "\n",
        "This approach is ideal, when we need to run quick experimentation or prototyping and don't need to gain more granular control on the model behavior."
      ],
      "metadata": {
        "id": "U1CpegdxhTXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Load the pipeline with the desired task and the model\n",
        "transcriber = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-medium.en\",\n",
        "    dtype=torch.float16,\n",
        "    device='cuda',\n",
        "    return_timestamps=True\n",
        ")"
      ],
      "metadata": {
        "id": "c-P2FTteKs6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application 1 - Print the audio transcript"
      ],
      "metadata": {
        "id": "zzcUb5BoxF7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to generate the transcription\n",
        "def transcribe(audio_filename):\n",
        "  if audio_filename is None:\n",
        "    return \"No audio file provided\"\n",
        "\n",
        "  # Run inference - Generate the transcript text by calling the model through the pipeline API\n",
        "  transcript = transcriber(audio_filename)[\"text\"]\n",
        "  # Return the transcript text\n",
        "  return transcript\n",
        "\n",
        "# Call custom function\n",
        "result1 = transcribe(audio_filename_1)\n",
        "# result2 = transcribe(audio_filename_2)\n",
        "\n",
        "# Print the transcription as markdown\n",
        "display(Markdown(result1))\n",
        "# display(Markdown(result2))"
      ],
      "metadata": {
        "id": "TgrRbQmjji0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application 2 - Display the audio transcript in Gradio UI"
      ],
      "metadata": {
        "id": "B-6pSNDmqZzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the UI with Gradio\n",
        "\n",
        "ui = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(\n",
        "        label=\"Upload audio file\",\n",
        "        sources=\"upload\", # Indicate file upload, which is default, or it can be microphone\n",
        "        type=\"filepath\" # Dedault is 'numpy' which converts the audio to a tuple consisting of the sample rate and the data\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Transcript\", lines=5),\n",
        "    title=\"Audio Transcription using Whisper from HuggingFace\",\n",
        "    description=\"Upload an audio file and get the transcript.\"\n",
        ")\n",
        "\n",
        "# Launch the UI\n",
        "ui.launch()"
      ],
      "metadata": {
        "id": "X1J3wz5d6Ovg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}