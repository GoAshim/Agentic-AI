{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Text Summarization Agent using Open Source LLM from HuggingFace\n",
        "\n",
        "This is the second notebook in the series of experiments where I will build different AI agents using open-source LLMs from HuggingFace.\n",
        "\n",
        "### Google Colab\n",
        "I will use Google Colab for creating and running the python code to build the AI agents using open-source LLMs from HuggingFace. Why did I choose Google Colab instead of my local computer?\n",
        "1. Free access to powerful T4 GPUs needed to run most of the LLMs efficiently.\n",
        "2. Easy ability to share code and collaborate.\n",
        "\n",
        "### Hugging Face\n",
        "I will need to connect to HuggingFace to use the appropriate open-source LLM for the AI application and connect that from my notebook in Colab. Here are the steps -\n",
        "1. Create a free HuggingFace account at https://huggingface.co\n",
        "2. Navigate to Settings from the user menu on the top right.\n",
        "3. Create a new API token with **write** permissions.\n",
        "4. Back to this colab notebook\n",
        "  * Press the \"key\" icon on the side panel to the left\n",
        "  * Click on add a new secret\n",
        "  * In the name field put HF_TOKEN\n",
        "  * In the value field put your actual token: hf_...\n",
        "  * Ensure the notebook access switch is turned ON.\n",
        "\n",
        "This way I can use my confidential API Keys for HuggingFace or others without needing to type them into my colab notebook, I will be sharing with others."
      ],
      "metadata": {
        "id": "kOIBb82S27cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and specifications, such as its memory usage, temperature, and clock speed.\n",
        "# We can also see that in details by clicking on Runtime (top menu) > View Resources\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "q84x9WTsADQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will need to connect from my notebook in Colab to HuggingFace by validating the token, in order to use open-source models.\n",
        "# The huggingface_hub library allows to interact with the HuggingFace Hub, a platform democratizing open-source LLMs and Datasets\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "378coGwPFpmD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection\n",
        "\n",
        "I will select a model from the HuggingFace model library based on the specific  application. Here are the steps -\n",
        "\n",
        "* Go to https://huggingface.co/models.\n",
        "* Click on Summarization under NLP.\n",
        "* Choose any model and review it's specification.\n",
        "* I am choosing the bart-large-cnn model from Facebook\n",
        "\n",
        "Note: We should select a model based on various criteria, such as the specific use-casr, available infrastructure, latency, performance. I will cover those in details later."
      ],
      "metadata": {
        "id": "Mg2jfrVvH5db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1 - Using HuggingFace Pipeline Library\n",
        "\n",
        "This is a much simpler approach with the Hugging Face pipeline API, which  provides a high-level, task-specific interface for running inference with pretrained models without manually handling tokenization, preprocessing, or postprocessing.\n",
        "\n",
        "This approach is ideal, when we need to run quick experimentation or prototyping and don't need to gain more granular control on the model behavior."
      ],
      "metadata": {
        "id": "U1CpegdxhTXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pipeline with the desired task and the model\n",
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")"
      ],
      "metadata": {
        "id": "c-P2FTteKs6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application 1\n",
        "\n",
        "# Provide the input text to summarize\n",
        "text_to_summarize = \"\"\"\n",
        "Model quantization makes it possible to deploy increasingly complex deep learning models in resource-constrained environments\n",
        "without sacrificing significant model accuracy. As AI models, especially generative AI models, grow in size and computational\n",
        "demands, quantization addresses challenges such as memory usage, inference speed, and energy consumption by reducing the\n",
        "precision of model parameters (weights and/or activations), e.g., from FP32 precision to FP8 precision. This reduction\n",
        "decreases the model’s size and computational requirements, enabling faster computation during inference and lower power\n",
        "consumption compared to the original model. However, quantization can lead to some accuracy degradation compared to the\n",
        "original model. Finding the right tradeoff between model accuracy and efficiency depends heavily on the specific use case.\n",
        "\"\"\"\n",
        "\n",
        "# Run inference\n",
        "summary = summarizer(text_to_summarize, max_length=100, min_length=50, do_sample=False)\n",
        "# max_length and min_length are indicated in number of tokens,\n",
        "# do_sample = false, by default, and is good for factual, consistent summaries\n",
        "# do_sample = true, when we need more creative summaries\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "TgrRbQmjji0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 2 - Using HuggingFace Tokenizer & Model Libraries\n",
        "\n",
        "This is more nuanced approach than the previous one with the HuggingFace pipeline API. Here, I will use the Tokenizer & Model Libraries from HuggingFace. Though this approach involves more coding, but it gives us more granular access to the model execution and the tokens, such as Token IDs, Attention masks, Model logits, etc.\n",
        "\n",
        "This approach is ideal, when we need to run either custom preprocessing, custom post‑processing, fine‑tuning, debugging model behavior or building our own pipeline."
      ],
      "metadata": {
        "id": "B-6pSNDmqZzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and the model library\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Load the specific tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "eiLqsjr-tR39"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application 1\n",
        "\n",
        "# Provide the input text to summarize\n",
        "text_to_summarize = \"\"\"\n",
        "Model quantization makes it possible to deploy increasingly complex deep learning models in resource-constrained environments\n",
        "without sacrificing significant model accuracy. As AI models, especially generative AI models, grow in size and computational\n",
        "demands, quantization addresses challenges such as memory usage, inference speed, and energy consumption by reducing the\n",
        "precision of model parameters (weights and/or activations), e.g., from FP32 precision to FP8 precision. This reduction\n",
        "decreases the model’s size and computational requirements, enabling faster computation during inference and lower power\n",
        "consumption compared to the original model. However, quantization can lead to some accuracy degradation compared to the\n",
        "original model. Finding the right tradeoff between model accuracy and efficiency depends heavily on the specific use case.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(\n",
        "    text_to_summarize,\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "# max_length indicates the max number of token the model can handle at a time.\n",
        "# Hence, in order to summarize larger content, we will need to split it into chunks and then summarize each chunk.\n",
        "\n",
        "# Run model to generate the output tokens\n",
        "summary_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=4,\n",
        "    min_length=30,\n",
        "    max_length=100\n",
        ")\n",
        "# num_beams indicates which helps generate higher quality token. Below are some options\n",
        "# num_beams=1, indicates Greedy Search, which is fast but has low quality\n",
        "# num_beams=2 to 4, indicates Light beam search, which is slightly slower but has better quality\n",
        "# num_beams=5 to 8, indicates Strong beam search, which is much slower but has best quality\n",
        "\n",
        "# Decode tokens back to text\n",
        "summary = tokenizer.batch_decode(\n",
        "    summary_ids,\n",
        "    skip_special_tokens=True,\n",
        "    truncation=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")[0]\n",
        "\n",
        "print({\"Summarized content: \": summary})"
      ],
      "metadata": {
        "id": "2ERJHekEuYsf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}