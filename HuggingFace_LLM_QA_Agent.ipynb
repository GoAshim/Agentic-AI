{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
          "state": {}
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Q&A Agent using Open Source LLM from HuggingFace\n",
        "\n",
        "This is the first notebook in the series of experiments where I will build different AI agents using open-source LLMs from HuggingFace.\n",
        "\n",
        "### Google Colab\n",
        "I will use Google Colab for creating and running the python code to build the AI agents using open-source LLMs from HuggingFace. Why did I choose Google Colab instead of my local computer?\n",
        "1. Free access to powerful T4 GPUs needed to run most of the LLMs efficiently.\n",
        "2. Easy ability to share code and collaborate.\n",
        "\n",
        "### Hugging Face\n",
        "I will need to connect to HuggingFace to use the appropriate open-source LLM for the AI application and connect that from my notebook in Colab. Here are the steps -\n",
        "1. Create a free HuggingFace account at https://huggingface.co\n",
        "2. Navigate to Settings from the user menu on the top right.\n",
        "3. Create a new API token with **write** permissions.\n",
        "4. Back to this colab notebook\n",
        "  * Press the \"key\" icon on the side panel to the left\n",
        "  * Click on add a new secret\n",
        "  * In the name field put HF_TOKEN\n",
        "  * In the value field put your actual token: hf_...\n",
        "  * Ensure the notebook access switch is turned ON.\n",
        "\n",
        "This way I can use my confidential API Keys for HuggingFace or others without needing to type them into my colab notebook, I will be sharing with others."
      ],
      "metadata": {
        "id": "kOIBb82S27cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and specifications, such as its memory usage, temperature, and clock speed.\n",
        "# We can also see that in details by clicking on Runtime (top menu) > View Resources\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q84x9WTsADQl",
        "outputId": "16b1473c-e3ad-4a6c-e729-c8d542526b8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 29 05:11:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I will need to connect from my notebook in Colab to HuggingFace by validating the token, in order to use open-source models.\n",
        "# The huggingface_hub library allows to interact with the HuggingFace Hub, a platform democratizing open-source LLMs and Datasets\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "378coGwPFpmD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection\n",
        "\n",
        "I will select a model from the HuggingFace model library based on the specific  application. Here are the steps -\n",
        "\n",
        "* Go to https://huggingface.co/models.\n",
        "* Click on Question Answering under NLP.\n",
        "* Choose any model and review it's specification.\n",
        "* I am choosing the model at https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad\n",
        "\n",
        "Note: We should select a model based on various criteria, such as the specific use-casr, available infrastructure, latency, performance. I will cover those in details later."
      ],
      "metadata": {
        "id": "Mg2jfrVvH5db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Approach 1 - Using HuggingFace Pipeline\n",
        "\n",
        "This is a much simpler approach with the Hugging Face pipeline API, which  provides a high-level, task-specific interface for running inference with pretrained models without manually handling tokenization, preprocessing, or postprocessing.\n",
        "\n",
        "This approach is ideal, when we need to run quick experimentation or prototyping and don't need to gain more granular control on the model behavior."
      ],
      "metadata": {
        "id": "U1CpegdxhTXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the QA pipeline with the desired task and the model\n",
        "qa = pipeline(\n",
        "    task=\"question-answering\",\n",
        "    model=\"distilbert/distilbert-base-cased-distilled-squad\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "af4aa900735c4fd587263f34ab3382a0",
            "c4463e7f10054acd9ddd62885dfa0a4a",
            "8c152809dacc4c0782bc07f801e0557c",
            "4e82949091064c92b35a216f2a6905c3",
            "2563cf37546149639750eebf888c6fcd",
            "da542dff057349a1845f82e410e14e29",
            "99058694bf6447e1a601c387d1927f71",
            "97c7709af1524b33ac85e68a9b520103",
            "4b72b89d86db49a0892ef1f50dbfc1bd",
            "60e31cf4871d4155838d78c11c541186",
            "312a4d9cb55745e49696fb8f5652f919",
            "bc4aff8db7344a2d82f61540b2e79c40",
            "f3d0072e675f4cefae8efd314f7f90e1",
            "cb7e1a8c81384c13a2b8fd8a60a09327",
            "25d19e1252a34a3ba1f01f31fa0c40cb",
            "257ae1ae48a540949e0d0fa5505c3e47",
            "ca445909ef1b40f0bd6dcf95bbe1407b",
            "2d166bba7cfe49178a9d84b7dd600231",
            "a25267c6dc064a49830afdc3955fe143",
            "d6c42c34f8c845d29fefe8390f619488",
            "9ad4df44b5cb4a32970ef29ed9f742ac",
            "90d587219067449db4e824888f84d6a3",
            "203efe58bd5540f790fb3972d57423e6",
            "f1e2ab7e903349a5bea861cea67b158a",
            "f60e5ad7a7e94e1bb53cbca882987095",
            "ad191d3aa66a42948189b7cba97da6b7",
            "a4bd46539533439d8b4cdd5c069e16c3",
            "f8ef99936e1048c7ade8228acc9b2fa2",
            "ce01023a43f24b69a67fd4d5bc466be7",
            "e89d60cb26e9472a997aa09712b14904",
            "5ea4b9be2595477ebb9edb82429d66c4",
            "5d01e582cc144364809ef2b4d84d15f7",
            "87fc4ff17896467ca46b43072eed516a",
            "af3964dde61e4bd3a7a95e490e39bf5c",
            "dcc669b7dba64bf28ccc0d37bc4a7a36",
            "208d08b51c22429d92c50334e7dfdce3",
            "acee5ac2686d4bfeb85a40b46bf80754",
            "8755e3f6b1a349179122433c0b1e8470",
            "04cc96832f394d7e88c7da9e29ca9de5",
            "d6f48de05d8d41069a5c6cb5d468c276",
            "889014e51c7e4d2e9b27d806cec3835a",
            "d749a5bf6274402991421c8fd9c0e059",
            "0afaca0eec704e959e569baeb517f404",
            "06455a07ab1240b1b719b9974e434310",
            "9bec981acd2e43f3a176f82908eaf382",
            "e2fc903eb43241c888b695012129ad64",
            "8f3155bd3e7748beaf4f05a711e4cb52",
            "211695f38af741c0936a3ff8714e7dbd",
            "3d4ed168e0524470ab430904c1ed450d",
            "33740b02fe2d4acc82983dce3db85586",
            "0415de6410c94a509a966b41af61fff0",
            "22a5ff038fe448c9ab28a5fb78ccd78f",
            "9f6cf3a6ede744509cb00d28bf09f1bd",
            "c03cfc2126d14c639e7b26187ccd471c",
            "dd8ee20fc16849b98aad996201f9ec70"
          ]
        },
        "id": "c-P2FTteKs6I",
        "outputId": "4b1f1736-227b-4f3b-dd32-c4ac912ba9cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af4aa900735c4fd587263f34ab3382a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc4aff8db7344a2d82f61540b2e79c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203efe58bd5540f790fb3972d57423e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af3964dde61e4bd3a7a95e490e39bf5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9bec981acd2e43f3a176f82908eaf382"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Application 1\n",
        "\n",
        "# Sample input with the context and the question, the answer of which is in the context\n",
        "question = \"Who wrote the play Hamlet?\"\n",
        "context = \"Hamlet is a famous tragedy written by William Shakespeare.\"\n",
        "\n",
        "# Run inference\n",
        "result = qa({\n",
        "    \"question\": question,\n",
        "    \"context\": context\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgrRbQmjji0j",
        "outputId": "2f1c00d5-f8ba-4557-c41a-ff9f66c668ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/pipelines/question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9790515899658203, 'start': 38, 'end': 57, 'answer': 'William Shakespeare'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Application 2\n",
        "\n",
        "# Sample input with the context and the question, the answer of which is in the context\n",
        "question = \"What is Sagrada Família?\"\n",
        "context = \"\"\"\n",
        "Barcelona boasts a history that stretches back to Roman times. Explore the narrow, winding streets of the Gothic Quarter,\n",
        "the ancient heart of the city, where medieval buildings, churches, and palaces whisper tales of centuries past. You can\n",
        "even see remnants of Roman walls incorporated into the city's fabric. Prepare to be amazed by Barcelona's architectural\n",
        "prowess. The city is a treasure trove of Modernist and Art Nouveau masterpieces. Antoni Gaudí's works are simply mesmerizing.\n",
        "The iconic Sagrada Família is an absolute must-see, a colossal temple that has become a symbol of the city. Don't miss his\n",
        "other incredible creations like Casa Batlló, Casa Milà (La Pedrera), and Güell Park, all designated as UNESCO World Heritage\n",
        "sites. Discover the stunning Music Palace, another UNESCO World Heritage site, showcasing the remarkable talent of architects\n",
        "like Luis Doménechi Montaner. Stroll hand-in-hand down Las Ramblas, Barcelona's most famous promenade. This lively boulevard,\n",
        "lined with trees, is a delightful spectacle of flower stalls, kiosks selling books and newspapers, and a vibrant street life\n",
        "that leads you down to the port and the impressive Columbus Monument. From its ancient core to the expansive L'Eixample\n",
        "district, designed with geometric blocks and open spaces, Barcelona showcases a fascinating evolution. While modern buildings\n",
        "add to the skyline, the city's soul lies in its historical layers and the artistic flair that permeates its streets.\n",
        "\"\"\"\n",
        "\n",
        "# Run inference\n",
        "result = qa({\n",
        "    \"question\": question,\n",
        "    \"context\": context\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSuQ-cS-kxxP",
        "outputId": "ada21a40-6b1f-451a-b859-8a9dec7ffe00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.291929692029953, 'start': 545, 'end': 562, 'answer': 'a colossal temple'}\n"
          ]
        }
      ]
    }
  ]
}